{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the Correctness of Exported Model and Compare the Performance\n",
    "\n",
    "We choose PyTorch to export the ONNX model, and use Caffe2 and Tensorflow as backend.\n",
    "After that, the outputs and performance of the three models are compared.\n",
    "\n",
    "The ONNX Tutorial \"Verify the Correctness of Exported Model and Compare the Performance\" uses only Caffe2 as backend. But it fails when running the Caffe2 Model. In this notebook, we used a workaround to correct this issue. There are also some drawbacks of \n",
    "\n",
    "Reference : https://github.com/onnx/tutorials/blob/master/tutorials/CorrectnessVerificationAndPerformanceComparison.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from caffe2.python import core\n",
    "from torch.autograd import Variable\n",
    "from caffe2.python.onnx.backend import Caffe2Backend\n",
    "from caffe2.python.onnx.helper import c2_native_run_net, save_caffe2_net, load_caffe2_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ONNX model from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch model.\n",
    "pytorch_model = MNIST()\n",
    "pytorch_model.train(False)\n",
    "\n",
    "# Generate dummy inputs.\n",
    "\n",
    "inputs = (Variable(torch.randn(3, 1, 28, 28), requires_grad=True), )\n",
    "\n",
    "# Run the PyTorch exporter to generate an ONNX model.\n",
    "torch.onnx.export(pytorch_model, inputs, \"mnist_pytorch.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the ONNX model.\n"
     ]
    }
   ],
   "source": [
    "# Load the onnx model\n",
    "onnx_model = onnx.load('mnist_pytorch.onnx')\n",
    "\n",
    "# Check whether the onnx_model is valid or not.\n",
    "print(\"Check the ONNX model.\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Caffe2 Model from ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have an ONNX model, let's turn it into a Caffe2 one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert the model to a Caffe2 model.\n"
     ]
    }
   ],
   "source": [
    "# Convert the ONNX model to a Caffe2 model.\n",
    "print(\"Convert the model to a Caffe2 model.\")\n",
    "init_net, predict_net = Caffe2Backend.onnx_graph_to_caffe2_net(onnx_model, device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caffe2 takes a list of numpy array as inputs. So we need to change the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inputs for Caffe2.\n",
    "caffe2_inputs = [var.data.numpy() for var in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TensorFlow Model from ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for TensorFlow\n",
    "tensorflow_inputs = [var.detach().numpy() for var in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ONNX model to a TensorFlow model.\n",
    "tf_rep = prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running different models\n",
    "\n",
    "Run PyTorch, Caffe2 and Tensorflow models separately, and get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the results using the PyTorch model.\n",
    "pytorch_results = pytorch_model(*inputs)\n",
    "\n",
    "# Compute the results using the Caffe2 model.\n",
    "# the function c2_native_run_net return 2 objects, the workspace and the results\n",
    "# ws is an object representing a Caffe2 workspace an instance of the class Workspace from onnx_caffe2.workspace,\n",
    "#This class makes it possible to work with workspaces more locally, and without forgetting \n",
    "#to deallocate everything in the end.\n",
    "ws, caffe2_results = c2_native_run_net(init_net, predict_net, caffe2_inputs)\n",
    "\n",
    "# Compute the results using the Tensorflow model.\n",
    "tensorflow_results = tf_rep.run(tensorflow_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness Check of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results, let's check the correctness of the exported model.\n",
    "If no assertion fails, our model has achieved expected precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The exported model achieves 5-decimal precision.\n"
     ]
    }
   ],
   "source": [
    "# Check the decimal precision of the exported Caffe2 and Tensorflow.\n",
    "expected_decimal = 5\n",
    "for p, c, t in zip([pytorch_results], caffe2_results, tensorflow_results):\n",
    "    np.testing.assert_almost_equal(p.data.cpu().numpy(), c, decimal=expected_decimal)\n",
    "    np.testing.assert_almost_equal(p.data.cpu().numpy(), t, decimal=expected_decimal)\n",
    "    np.testing.assert_almost_equal(c, t, decimal=expected_decimal)\n",
    "print(\"The exported model achieves {}-decimal precision.\".format(expected_decimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Check of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code measures the performance of PyTorch, Caffe2 and Tensorflow models.\n",
    "We report:\n",
    "- Execution time per iteration\n",
    "- Iterations per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a function that measures PyTorch model performance (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_pytorch_model(model, inputs, warmup_iters=3, main_iters=10):\n",
    "    '''\n",
    "     Run the model several times, and measure the execution time.\n",
    "     Print the execution time per iteration (millisecond) and the number of iterations per second.\n",
    "    '''\n",
    "    for _i in range(warmup_iters):\n",
    "        model(*inputs)\n",
    "        \n",
    "    total_time = 0.0\n",
    "    \n",
    "    for _i in range(main_iters):\n",
    "        ts = time.time()\n",
    "        model(*inputs)\n",
    "        te = time.time()\n",
    "        total_time += te - ts\n",
    "    \n",
    "    print(\"The PyTorch model execution time per iter is {} milliseconds, \"\n",
    "          \"{} iters per second.\".format(total_time / main_iters * 1000,\n",
    "                                        main_iters / total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a function that measures Caffe2 model performance\n",
    "\n",
    "We decided to use this method instead of the builtin function benchmark_caffe2_model under caffe2.python.onnx.helper so that the same way of benchmarking would be apllied on all the frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " def performance_caffe2_model(init_net, predict_net,inputs, warmup_iters=3, main_iters=10):\n",
    "    '''\n",
    "     Run the model several times, and measure the execution time.\n",
    "     Print the execution time per iteration (millisecond) and the number of iterations per second.\n",
    "    '''\n",
    "    for _i in range(warmup_iters):\n",
    "        ws, caffe2_results = c2_native_run_net(init_net, predict_net, inputs)    \n",
    "    \n",
    "    total_time = 0.0\n",
    "    for _i in range(main_iters):\n",
    "        ts = time.time()\n",
    "        ws, caffe2_results = c2_native_run_net(init_net, predict_net, inputs)\n",
    "        te = time.time()\n",
    "        total_time += te - ts\n",
    "    print(\"The Caffe2 model execution time per iter is {} milliseconds, \"\n",
    "          \"{} iters per second.\".format(total_time / main_iters * 1000,\n",
    "                                        main_iters / total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a function that measures Tensorflow model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " def performance_tensorflow_model(model, inputs, warmup_iters=3, main_iters=10):\n",
    "    '''\n",
    "     Run the model several times, and measure the execution time.\n",
    "     Print the execution time per iteration (millisecond) and the number of iterations per second.\n",
    "    '''\n",
    "\n",
    "    for _i in range(warmup_iters):\n",
    "        output = tf_rep.run(inputs)\n",
    "    \n",
    "    total_time = 0.0\n",
    "    for _i in range(main_iters):\n",
    "        ts = time.time()\n",
    "        output = tf_rep.run(inputs)\n",
    "        te = time.time()\n",
    "        total_time += te - ts\n",
    "    print(\"The Tensorflow model execution time per iter is {} milliseconds, \"\n",
    "          \"{} iters per second.\".format(total_time / main_iters * 1000,\n",
    "                                        main_iters / total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PyTorch model execution time per iter is 0.6457304954528809 milliseconds, 1548.633690125868 iters per second.\n"
     ]
    }
   ],
   "source": [
    "performance_pytorch_model(pytorch_model, inputs,warmup_iters=3,main_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Caffe2 model execution time per iter is 2.544734477996826 milliseconds, 392.96830716389076 iters per second.\n"
     ]
    }
   ],
   "source": [
    "performance_caffe2_model(init_net, predict_net,caffe2_inputs, warmup_iters=3, main_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tensorflow model execution time per iter is 18.066928386688232 milliseconds, 55.3497516897672 iters per second.\n"
     ]
    }
   ],
   "source": [
    "performance_tensorflow_model(onnx_model, tensorflow_inputs, warmup_iters=3,main_iters=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
